{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571b5e88",
   "metadata": {},
   "source": [
    "### Imports and Prerequisite Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from toeplitzlda.classification import ToeplitzLDA\n",
    "import pyxdf\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "def load_and_preprocess_raw(header_file, filter_band=(0.5, 16)):\n",
    "    non_eeg_channels = [\"EOGvu\", \"x_EMGl\", \"x_GSR\", \"x_Respi\", \"x_Pulse\", \"x_Optic\"]\n",
    "    raw = mne.io.read_raw_brainvision(header_file, misc=non_eeg_channels, preload=True)\n",
    "    raw.set_montage(\"standard_1020\")\n",
    "    raw.filter(*filter_band, method=\"iir\")\n",
    "    raw.pick_types(eeg=True)\n",
    "    return raw\n",
    "\n",
    "def load_and_preprocess_xdf(xdf_file, filter_band):\n",
    "    # Load without dejittering anything yet\n",
    "    data, _ = pyxdf.load_xdf(xdf_file, dejitter_timestamps=False)\n",
    "\n",
    "    # Get stream names\n",
    "    stream_names = [stream['info']['name'][0] for stream in data]\n",
    "\n",
    "    # Find EEG stream\n",
    "    idx_eeg = stream_names.index(\"BrainAmp BUA -\")\n",
    "    eeg_stream = data[idx_eeg]\n",
    "\n",
    "    eeg_data = np.array(eeg_stream['time_series']).T  # shape: (n_channels, n_samples)\n",
    "    ts_eeg = np.array(eeg_stream['time_stamps'])\n",
    "    info_eeg = eeg_stream['info']\n",
    "\n",
    "    # Find Marker stream\n",
    "    idx_markers = stream_names.index(\"AuditoryAphasiaAudioMarker\")\n",
    "    markers = [int(m[0]) for m in data[idx_markers]['time_series']]\n",
    "    time_stamps = np.array(data[idx_markers]['time_stamps'])\n",
    "\n",
    "    # Create info for MNE\n",
    "    ch_names = [ch['label'][0] for ch in info_eeg['desc'][0]['channels'][0]['channel']]\n",
    "    sfreq = float(info_eeg['nominal_srate'][0])\n",
    "    ch_types = ['eeg'] * len(ch_names)\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "\n",
    "    onsets = time_stamps - ts_eeg[0]\n",
    "\n",
    "    # Create RawArray\n",
    "    raw = mne.io.RawArray(eeg_data, info)\n",
    "\n",
    "    annotations = mne.Annotations(\n",
    "        onset=onsets,\n",
    "        duration=[0] * len(time_stamps),\n",
    "        description=markers\n",
    "    )\n",
    "\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # FIX the channel type\n",
    "    if 'MkIdx' in raw.ch_names:\n",
    "        raw.set_channel_types({'MkIdx': 'stim'})  # or 'misc'\n",
    "\n",
    "    # Preprocessing\n",
    "    raw.set_montage(\"standard_1020\")\n",
    "    raw.filter(*filter_band, method=\"iir\")\n",
    "    raw.pick_types(eeg=True)\n",
    "\n",
    "    return raw\n",
    "\n",
    "def epoch_raw(raw, decimate=10):\n",
    "    id_map = {\n",
    "        '101': 101,\n",
    "        '102': 102,\n",
    "        '103': 103,\n",
    "        '104': 104,\n",
    "        '105': 105,\n",
    "        '106': 106,\n",
    "        '111': 111,\n",
    "        '112': 112,\n",
    "        '113': 113,\n",
    "        '114': 114,\n",
    "        '115': 115,\n",
    "        '116': 116,\n",
    "        '201': 201,\n",
    "        '202': 202,\n",
    "        '203': 203,\n",
    "        '204': 204,\n",
    "        '205': 205,\n",
    "        '206': 206,\n",
    "    }\n",
    "\n",
    "    evs, _ = mne.events_from_annotations(raw, event_id=id_map)\n",
    "    target_ids = list(range(111, 117))\n",
    "    non_target_ids = list(range(101, 107))\n",
    "    event_id = {f\"Word_{i - 110}/Target\": i for i in target_ids}\n",
    "    event_id.update({f\"Word_{i - 100}/NonTarget\": i for i in non_target_ids})\n",
    "    epoch = mne.Epochs(raw, events=evs, event_id=event_id, decim=decimate,\n",
    "                       proj=False, tmax=1, baseline=None)\n",
    "    return epoch\n",
    "\n",
    "def load_data(participant, task=\"Default\", filter_band=(0.5, 16)):\n",
    "    data_dir = Path.cwd() / \"session_data\" / participant / \"run\" / f\"sub-{participant}\" / \"ses-S001\" / \"eeg\"\n",
    "    \n",
    "    xdf_files = sorted(data_dir.glob(f\"sub-{participant}_ses-S001_task-{task}_run-*_eeg.xdf\"))\n",
    "\n",
    "    if not xdf_files:\n",
    "        print(f\"No XDF files found in {data_dir}\")\n",
    "        return []\n",
    "    \n",
    "        # Load the data, preprocess and slice it into epochs\n",
    "    epochs = list()\n",
    "    for xdf_file in xdf_files:\n",
    "        raw_data = load_and_preprocess_xdf(xdf_file, filter_band)\n",
    "        epochs.append(epoch_raw(raw_data))\n",
    "\n",
    "    # Overwrite epochs list to save memory\n",
    "    epochs = mne.concatenate_epochs(epochs)\n",
    "\n",
    "    # Combine 6 epochs into a single iteration (6 stimuli together form a single iteration)\n",
    "    iterations = [epochs[i:i+6] for i in np.arange(0, epochs.events.shape[0],6)]\n",
    "\n",
    "    # Assert that each iteration contains exactly 1 Target\n",
    "    assert all([len(iteration[\"Target\"]) == 1 for iteration in iterations]), \"Number of targets in single iterations is unequal to 1.\"\n",
    "\n",
    "    # 15 iterations form a single trial\n",
    "    trials = [iterations[i:i+15] for i in np.arange(0,len(iterations),15)]\n",
    "\n",
    "    return trials\n",
    "    \n",
    "def get_jumping_means(epo, boundaries):\n",
    "    shape_orig = epo.get_data().shape\n",
    "    X = np.zeros((shape_orig[0], shape_orig[1], len(boundaries)-1))\n",
    "    for i in range(len(boundaries)-1):\n",
    "        idx = epo.time_as_index((boundaries[i], boundaries[i+1]))\n",
    "        idx_range = list(range(idx[0], idx[1]))\n",
    "        X[:,:,i] = epo.get_data()[:,:,idx_range].mean(axis=2)\n",
    "    return X\n",
    "\n",
    "def compute_auc_per_config(trials, clf_ival_boundaries):\n",
    "    results = np.zeros(len(trials))\n",
    "    for t, trial in enumerate(trials):\n",
    "        features, labels = list(), list()\n",
    "        for iteration in trial:\n",
    "            for s, stimulus in enumerate(iteration):\n",
    "                features.extend([\n",
    "                    get_jumping_means(iteration[s], clf_ival_boundaries).squeeze().flatten()\n",
    "                ])\n",
    "            labels.extend([\n",
    "                1 if event > 107 else 0\n",
    "                for event in iteration.events[:,2]\n",
    "            ])\n",
    "        X = np.array(features)\n",
    "        # print(f\"X shape: {X.shape}\")\n",
    "        y = np.array(labels)\n",
    "        n_channels = trial[0].get_data().shape[1]\n",
    "        clf = ToeplitzLDA(n_channels=n_channels, data_is_channel_prime=False)\n",
    "        skf = StratifiedKFold(n_splits=15, shuffle=False)\n",
    "        auc_scores = cross_val_score(clf, X, y, cv=skf, scoring='roc_auc')\n",
    "        results[t] = auc_scores.mean()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5712ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = [\"VPpdla\", \"VPpdlb\", \"VPpdlc\", \"VPpdld\", \"VPpdlf\"]\n",
    "step = 0.05\n",
    "clf_ival_boundaries = np.arange(0.1, 0.81, step)\n",
    "print(clf_ival_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25f72f",
   "metadata": {},
   "source": [
    "### Validation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of validation phase data\n",
    "val_auc_scores = np.zeros((5,24))\n",
    "for i, participant in enumerate(participants):\n",
    "    print(f\"Loading data of {participant}\")\n",
    "    trials = load_data(participant, task=\"Validation\")\n",
    "    print(\"Starting classification...\")\n",
    "    val_auc_scores[i,:] = compute_auc_per_config(trials, clf_ival_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "optimal_aucs = list()\n",
    "default_aucs = list()\n",
    "\n",
    "\n",
    "# Statistical testing for each participant\n",
    "for i, participant in enumerate(participants):\n",
    "    split_auc = np.split(val_auc_scores[i,:], 4) # -> assume Optimal -> Default -> Optimal -> Default\n",
    "    optimal_auc = np.concatenate([split_auc[0], split_auc[2]])\n",
    "    default_auc = np.concatenate([split_auc[1], split_auc[3]])\n",
    "    stat, p_value = wilcoxon(optimal_auc, default_auc, alternative=\"greater\")\n",
    "    print(participant)\n",
    "    print(f\"Optimal-EQ - AUC mean: {np.mean(optimal_auc):.3f}\", f\"AUC std: {np.std(optimal_auc):.3f}\")\n",
    "    print(f\"Default-EQ - AUC mean: {np.mean(default_auc):.3f}\", f\"AUC std: {np.std(default_auc):.3f}\")\n",
    "    print(p_value, p_value < 0.05)\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    optimal_aucs.append(optimal_auc)\n",
    "    default_aucs.append(default_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1cb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# \n",
    "fig, axs = plt.subplots(2, 3, figsize=(14, 8), sharex=True, sharey=True)\n",
    "axs = axs.flat\n",
    "axs[5].set_axis_off()\n",
    "\n",
    "for i in range(5):\n",
    "    axs[i].hist(default_aucs[i], bins=10, alpha=0.5, label='Default-EQ', color='darkorange')\n",
    "    axs[i].hist(optimal_aucs[i], bins=10, alpha=0.5, label='Optimal-EQ', color='royalblue')\n",
    "    axs[i].set_title(f'{participants[i]}')\n",
    "    axs[i].set_xlabel('AUC')\n",
    "    axs[i].grid(True)\n",
    "    axs[i].tick_params(labelbottom=True)\n",
    "    if i == 0 or i == 3:\n",
    "        axs[i].set_ylabel('Frequency')\n",
    "        # \n",
    "\n",
    "axs[0].legend()\n",
    "plt.suptitle('AUC Distributions of Optimal-EQ vs. Default-EQ per participant')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc250d39",
   "metadata": {},
   "source": [
    "### Loudness Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify optimization data\n",
    "opt_auc_scores = np.zeros((5,60))\n",
    "for i, participant in enumerate(participants):\n",
    "    print(f\"Loading data of {participant}\")\n",
    "    trials = load_data(participant, task=\"Default\")\n",
    "    print(\"Starting classification...\")\n",
    "    opt_auc_scores[i,:] = compute_auc_per_config(trials, clf_ival_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05354531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Parse log files for history of words played\n",
    "\n",
    "def parse_log_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    gain_pattern = re.compile(r'Gain configuration (\\d+) sampled: \\[([^\\]]+)\\]')\n",
    "    trial_pattern = re.compile(r'Trial (\\d+)')\n",
    "    word_pattern = re.compile(\n",
    "        r'Word (?P<word>\\w+?)_(?:target|non_target) played from direction (?P<direction>\\d) - (?P<time>[0-9.]+)'\n",
    "    )\n",
    "\n",
    "    parsed_data = []\n",
    "    current_trial = None\n",
    "    current_gain = None\n",
    "    seen_pairs = set()\n",
    "\n",
    "    for line in lines:\n",
    "        if \"end optimization\" in line.lower():\n",
    "            break  # Stop parsing here\n",
    "\n",
    "        trial_match = trial_pattern.search(line)\n",
    "        if trial_match:\n",
    "            current_trial = int(trial_match.group(1))\n",
    "            seen_pairs = set()  # reset per trial\n",
    "\n",
    "        gain_match = gain_pattern.search(line)\n",
    "        if gain_match:\n",
    "            config_id, gains = gain_match.groups()\n",
    "            current_gain = list(map(float, gains.split()))\n",
    "\n",
    "        word_match = word_pattern.search(line)\n",
    "        if word_match and current_trial is not None and current_gain is not None:\n",
    "            word = word_match.group(\"word\")\n",
    "            direction = int(word_match.group(\"direction\"))\n",
    "            if (word, direction) not in seen_pairs:\n",
    "                parsed_data.append({\n",
    "                    \"trial\": current_trial,\n",
    "                    \"word\": word,\n",
    "                    \"direction\": direction,\n",
    "                    \"gains\": current_gain.copy()\n",
    "                })\n",
    "                seen_pairs.add((word, direction))\n",
    "\n",
    "    return pd.DataFrame(parsed_data)\n",
    "\n",
    "\n",
    "for i, participant in enumerate(participants):\n",
    "    run_path = log_file_path = Path.cwd() / \"session_data\" / participant / \"run\" \n",
    "    log_file_path = run_path / \"opt_log.log\"\n",
    "    \n",
    "    df = parse_log_file(log_file_path)\n",
    "\n",
    "    # Account for wrong blocks\n",
    "    if participant == \"VPpdla\":\n",
    "        block = 10\n",
    "        start_idx = (block - 1) * 36  \n",
    "        end_idx = start_idx + 36  \n",
    "        df = df.drop(index=range(start_idx, end_idx)).reset_index(drop=True)\n",
    "    elif participant == \"VPpdlb\":\n",
    "        block = 1\n",
    "        start_idx = (block - 1) * 36  \n",
    "        end_idx = start_idx + 36  \n",
    "        df = df.drop(index=range(start_idx, end_idx)).reset_index(drop=True)\n",
    "    elif participant == \"VPpdlc\":\n",
    "        block = 2\n",
    "        start_idx = (block - 1) * 36  \n",
    "        end_idx = start_idx + 36  \n",
    "        df = df.drop(index=range(start_idx, end_idx)).reset_index(drop=True)\n",
    "\n",
    "    df.to_csv(run_path / \"parsed_word_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import numpy as np\n",
    "import pyloudnorm as pyln\n",
    "from eq import EQ\n",
    "\n",
    "def load_and_eq_word(word, direction, gains, audio_base_path, eq, fs=44100):\n",
    "    \"\"\"\n",
    "    Loads a word audio file using the wave module, applies EQ, and calculates LUFS.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The word (e.g., \"Knuffel\")\n",
    "        direction (int): Direction index (e.g., 1 → 1.wav)\n",
    "        gains (list of float): EQ gain values\n",
    "        audio_base_path (str): Path to the folder containing word folders\n",
    "        fs (int): Expected sampling rate\n",
    "\n",
    "    Returns:\n",
    "        processed_audio (np.ndarray): EQ'd audio (int16)\n",
    "        lufs (float): Integrated loudness in LUFS\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(audio_base_path, word, f\"{direction}.wav\")\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {filepath}\")\n",
    "\n",
    "    # Open wave file\n",
    "    with wave.open(filepath, 'rb') as wf:\n",
    "        n_channels = wf.getnchannels()\n",
    "        sampwidth = wf.getsampwidth()\n",
    "        sr = wf.getframerate()\n",
    "        n_frames = wf.getnframes()\n",
    "        raw_data = wf.readframes(n_frames)\n",
    "\n",
    "    if sr != fs:\n",
    "        raise ValueError(f\"Expected sampling rate {fs}, got {sr}\")\n",
    "\n",
    "    if sampwidth != 2:\n",
    "        raise ValueError(\"Only 16-bit PCM WAV files are supported.\")\n",
    "\n",
    "    # Convert byte data to int16 numpy array\n",
    "    audio_np = np.frombuffer(raw_data, dtype=np.int16).reshape(-1, n_channels)\n",
    "    eq.update_filter_gains(gains)\n",
    "    processed_audio = eq.apply_eq(audio_np, n_channels=n_channels)\n",
    "\n",
    "    # Compute LUFS (normalize to float [-1, 1] first)\n",
    "    meter = pyln.Meter(fs, block_size= 0.100)\n",
    "    mono = processed_audio.mean(axis=1) / 32768.0\n",
    "    lufs = meter.integrated_loudness(mono)\n",
    "\n",
    "    return processed_audio, lufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate LUFS for each participant\n",
    "\n",
    "freqs = [300, 500, 750]\n",
    "mod_indices = [[0],[1], [2]]\n",
    "eq = EQ(fs = 44100, freqs = freqs, mod_indices = mod_indices)\n",
    "\n",
    "lufs_all = np.zeros((5,60))\n",
    "\n",
    "for i, participant in enumerate(participants):\n",
    "    run_path = log_file_path = Path.cwd() / \"session_data\" / participant / \"run\" \n",
    "\n",
    "    lufs_arr = []\n",
    "\n",
    "    df = pd.read_csv(run_path / \"parsed_word_data.csv\")  # adjust filename if needed\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        direction = row[\"direction\"]\n",
    "        gains = eval(row[\"gains\"]) if isinstance(row[\"gains\"], str) else row[\"gains\"]  # ensure list\n",
    "        try:\n",
    "            processed_audio, _lufs = load_and_eq_word(\n",
    "                word=word,\n",
    "                direction=direction,\n",
    "                eq=eq,\n",
    "                gains=gains,\n",
    "                audio_base_path=\"stereo/\"  # replace with your actual path\n",
    "            )\n",
    "            lufs_arr.append(_lufs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {word} dir {direction} in trial {row['trial']}: {e}\")\n",
    "\n",
    "    lufs_all[i,:] = np.array(lufs_arr).reshape(-1, 6).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313dfda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('opt.npy', opt_auc_scores)\n",
    "# np.save('val.npy', val_auc_scores)\n",
    "\n",
    "opt_auc_scores = np.load(\"opt.npy\")\n",
    "val_auc_scores = np.load(\"val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 2, figsize=(8, 20), sharex=False, sharey=False)\n",
    "fig.suptitle(\"LUFS vs. AUC score and ΔLUFS vs. ΔAUC per participant\", fontsize=16)\n",
    "\n",
    "for i, participant in enumerate(participants):\n",
    "    auc_scores = opt_auc_scores[i]\n",
    "    lufs = lufs_all[i]\n",
    "    # print(lufs)\n",
    "    delta_auc = np.diff(auc_scores)\n",
    "    delta_lufs = np.diff(lufs)\n",
    "\n",
    "    # Plot 1: Mean LUFS vs AUC\n",
    "    slope, intercept = np.polyfit(lufs, auc_scores, 1)\n",
    "    x_vals = np.linspace(lufs.min(), lufs.max(), 100)\n",
    "    y_vals = slope * x_vals + intercept\n",
    "    r, p = pearsonr(lufs, auc_scores)\n",
    "\n",
    "    ax1 = axes[i, 0] \n",
    "    ax1.scatter(lufs, auc_scores, c='royalblue')\n",
    "    ax1.plot(x_vals, y_vals, color='red', linestyle='--', label='Fit line')\n",
    "    ax1.set_title(f\"LUFS vs. AUC\\n(r = {r:.2f}, p = {p:.2f}, AUC $\\sigma$ = {np.std(auc_scores):.2f})\")\n",
    "    ax1.set_xlabel(f\"LUFS\")\n",
    "    ax1.set_ylabel(f\"{participant}\\nAUC Score\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot 2: ΔLUFS vs ΔAUC\n",
    "    slope, intercept = np.polyfit(delta_lufs, delta_auc, 1)\n",
    "    x_vals = np.linspace(delta_lufs.min(), delta_lufs.max(), 100)\n",
    "    y_vals = slope * x_vals + intercept\n",
    "    dr, p = pearsonr(delta_lufs, delta_auc)\n",
    "\n",
    "    ax2 = axes[i, 1] \n",
    "    ax2.scatter(delta_lufs, delta_auc, c='royalblue')\n",
    "    ax2.plot(x_vals, y_vals,  color='red', linestyle='--', label='Fit line')\n",
    "    ax2.set_title(f\"ΔLUFS vs. ΔAUC\\n(r = {dr:.2f}, p = {p:.2f}, ΔAUC $\\sigma$ = {np.std(delta_auc):.2f})\")\n",
    "    ax2.set_xlabel(\"ΔLUFS\")\n",
    "    ax2.set_ylabel(\"ΔAUC\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f69e91",
   "metadata": {},
   "source": [
    "### Additional Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(14, 8), sharex=True, sharey=True)\n",
    "axs = axs.flat\n",
    "axs[5].set_axis_off()\n",
    "\n",
    "for i, participant in enumerate(participants):\n",
    "    run_path = log_file_path = Path.cwd() / \"session_data\" / participant / \"run\" \n",
    "    auc_per_config = np.load(run_path / \"auc_per_config.npy\")\n",
    "    axs[i].hist(auc_per_config, bins=10, color='royalblue')\n",
    "    axs[i].set_title(f'{participants[i]} ($\\mu = {np.mean(auc_per_config):.2f}$, $\\sigma$ = {np.std(auc_per_config):.2f})')\n",
    "    axs[i].set_xlabel('AUC')\n",
    "    axs[i].grid(True)\n",
    "    axs[i].tick_params(labelbottom=True)\n",
    "    if i == 0 or i == 3:\n",
    "        axs[i].set_ylabel('Frequency')\n",
    "        # \n",
    "\n",
    "axs[0].legend()\n",
    "plt.suptitle('Distribution of AUC scores per trial for each participant')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d88bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP plots (appendix)\n",
    "\n",
    "import optimization\n",
    "\n",
    "l_bounds = [1.0, 3.0, 2.5, 2.5, 2.5]\n",
    "optimal_configs = [[-2.816, -3.061, -5.755],[-2.816,-1.592,3.796],[-2.081,0.857,-5.755],[-3.796,-0.367,5.755],[-6, -0.122,2.327]]\n",
    "\n",
    "for i, participant in enumerate(participants):\n",
    "    # participant = \"VPpdlf\"\n",
    "    run_path = log_file_path = Path.cwd() / \"session_data\" / participant / \"run\" \n",
    "\n",
    "    configs = np.load(run_path / \"configs.npy\")\n",
    "    auc_per_config = np.load(run_path / \"auc_per_config.npy\")\n",
    "\n",
    "    gp, auc_scores_gp = optimization.fit_gp(configs, auc_per_config, lower_bound=l_bounds[i])\n",
    "    optimization.plot_gp(gp, lower_bound=l_bounds[i], best_config=optimal_configs[i], participant=participant, slice_axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb958f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Band gain plots\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(12, 20), sharex='col', sharey='row')\n",
    "fig.suptitle('Band gain vs. AUC score per participant', fontsize=16)\n",
    "\n",
    "for i, participant in enumerate(participants):\n",
    "    run_path = Path.cwd() / \"session_data\" / participant / \"run\"\n",
    "    configs = np.load(run_path / \"configs.npy\")\n",
    "    auc_scores = opt_auc_scores[i]  # shape: (n_configs,)\n",
    "\n",
    "    for band in range(3):\n",
    "        band_gains = configs[:, band]\n",
    "        ax = axes[i, band] \n",
    "\n",
    "        # Scatter plot\n",
    "        ax.scatter(band_gains, auc_scores, label='Data')\n",
    "\n",
    "        # Fit line\n",
    "        slope, intercept = np.polyfit(band_gains, auc_scores, deg=1)\n",
    "        x_vals = np.linspace(band_gains.min(), band_gains.max(), 100)\n",
    "        y_vals = slope * x_vals + intercept\n",
    "        ax.plot(x_vals, y_vals, color='red', linestyle='--', label='Fit line')\n",
    "\n",
    "        # Correlation coefficient\n",
    "        r, p = pearsonr(band_gains, auc_scores)\n",
    "        ax.set_title(f'Band {band + 1}\\n(r = {r:.2f}, p = {p:.2f})')\n",
    "\n",
    "        # Titles and labels\n",
    "        if band == 0:\n",
    "            ax.set_ylabel(f'{participant}\\nAUC Score')\n",
    "        ax.set_xlabel('Band gain (dB)')\n",
    "        ax.grid(True)\n",
    "        ax.tick_params(labelbottom=True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bscthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
